{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Self Attention and Transformer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbQ-SqIRmKxN"
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE0LdWY8m88B"
      },
      "source": [
        "Self attention layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayg6BGHaqNRK"
      },
      "source": [
        "torch.bmm(input, mat2, *, deterministic=False, out=None) → Tensor\n",
        "Performs a batch matrix-matrix product of matrices stored in input and mat2.\n",
        "\n",
        "input and mat2 must be 3-D tensors each containing the same number of matrices.\n",
        "\n",
        "If input is a $(b \\times n \\times m)$ tensor, mat2 is a $(b \\times m \\times p)$ tensor, out will be a $(b \\times n \\times p)$ tensor.\n",
        "\n",
        "$\\text{out}_i = \\text{input}_i \\mathbin{@} \\text{mat2}_i\n",
        "out_i =input_i i @mat2 i$\n",
        "​\t"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMzAnf9Wmq_f"
      },
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, k, heads=8):\n",
        "    super(SelfAttention, self).__init__()\n",
        "    self.k, self.heads = k, heads\n",
        "\n",
        "    self.tokeys = nn.Linear(k, k*heads, bias=False)\n",
        "    self.toqueries = nn.Linear(k, k*heads, bias=False)\n",
        "    self.tovalues = nn.Linear(k,k*heads, bias=False)\n",
        "    self.unifyheads = nn.Linear(k*heads, k)\n",
        "\n",
        "    def forward(self, x):\n",
        "      b, t, k = x.size()\n",
        "      h = self.heads\n",
        "\n",
        "      queries = self.toqueries(x).view(b,t,h,k)\n",
        "      keys    = self.tokeys(x).view(b,t,h,k)\n",
        "      values  = self.tovalues(x).view(b,t,h,k)\n",
        "\n",
        "      # fold heads into the batch dimension\n",
        "      queries = queries.transpose(1,2).contiguous.view(b*h, t,k)\n",
        "      keys = keys.transpose(1,2).contiguous.view(b*h, t, k)\n",
        "      values = values.transpose(1,2).contiguous.view(b*h, t, k)\n",
        "\n",
        "      # normalization \n",
        "      queries = queries/(k**(1/4))\n",
        "      keys = key/(k**(1/4))\n",
        "\n",
        "      # dot product of queries and keys to get W_{ij}\n",
        "      dot = torch.bmm(queries, keys.transpose(1,2))\n",
        "      # apply soft max ofer the w_ij \n",
        "      dot = F.softmax(dot, dim=2)\n",
        "      \n",
        "      # apply self attantion to the values \n",
        "      out = torch.bmm(dot, values).view(b,h,t,k)\n",
        "      # swapback h,t \n",
        "      out = out.transpose(1,2).view(b,t,h*k)\n",
        "\n",
        "      # unify the heads for output \n",
        "      unifyheads = self.unifyheads(out)\n",
        "\n",
        "      return unifyheads \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpBcXur2_QMC"
      },
      "source": [
        "simple transformer\n",
        "inputs -> self attention -> normalization -> MLPs -> normalizatin -> outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1qzSwldnZY7"
      },
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, k, heads):\n",
        "    super(TransformerBlock,self).__init__()\n",
        "    \n",
        "    self.attention = SelfAttention(k, heads=heads)\n",
        "\n",
        "    self.norm1 = nn.LinearNorm(k)\n",
        "    self.norm2 = nn.LinearNorm(k)\n",
        "\n",
        "    self.ff = nn.Sequential(\n",
        "        nn.Linear(k, 4*k),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*k,k)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    attention = self.attention(x)\n",
        "    x = self.norm1(attention + x)\n",
        "    forward = nn.ff(x)\n",
        "\n",
        "    return self.norm2(forward + x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmuGMEYNQ5aL"
      },
      "source": [
        "classification transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MyQYtQrF9_l"
      },
      "source": [
        "class Trasformer(nn.Module):\n",
        "  def __init__(self, k, heads, depth, seq_length, num_tokens, num_class):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.num_tokens = num_tokens\n",
        "    self.token_emb = nn.Embedding(num_embeddings=num_tokens, embedding_dim=k)\n",
        "    self.pos_emb = nn.Embedding(num_embeddings=seq_length, embedding_dim=k)\n",
        "\n",
        "    tblocks = []\n",
        "    for i in range(depth):\n",
        "      tblocks.append(TransformerBlock(k=k, heads=heads))\n",
        "    self.tblocks = nn.Sequential(*tblocks)\n",
        "\n",
        "    self.toprobs = nn.Linear(k, num_class)\n",
        "\n",
        "  def forward(self, x): \n",
        "    tokens = self.token_emb(x)\n",
        "    b, t, k = tokens.size()\n",
        "\n",
        "    positions = torch.Tensor(t)\n",
        "    positions = self.pos_emb(positions)[none,:,:].expand(b,t,k)\n",
        "\n",
        "    x = tokens + positions \n",
        "    x = self.tblocks(x)\n",
        "    x = x.mean(dim=1)\n",
        "    x = self.toprobs(x)\n",
        "\n",
        "    return F.log_softmax(x, dim=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WEY7em1Y822"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}